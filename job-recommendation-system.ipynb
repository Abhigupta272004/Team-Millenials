{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "   # for filename in filenames:\n",
    "     #   print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('/kaggle/input/job-recommendation-datasets/Combined_Jobs_Final.csv')\n",
    "df.head(3) # understanding first 3 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info() # info that what all columns are about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.646705Z",
     "iopub.status.busy": "2024-04-18T12:13:26.646212Z",
     "iopub.status.idle": "2024-04-18T12:13:26.663021Z",
     "shell.execute_reply": "2024-04-18T12:13:26.662126Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.646671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We will use Content-based recommendation system :- It uses tags, descriptions\n",
    "# we will be using only 2 columns Title, Job Description column.\n",
    "df_old=df[['Title','Job.Description']]\n",
    "df_old # df dataframe will be our new dataset and we will be working on this dataset only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.664404Z",
     "iopub.status.busy": "2024-04-18T12:13:26.664139Z",
     "iopub.status.idle": "2024-04-18T12:13:26.674249Z",
     "shell.execute_reply": "2024-04-18T12:13:26.673425Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.664381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# can reduce no. of rows so that it can work on hardware\n",
    "#df=df_old.sample(n=1000,random_state=0)  # out of 84089 rows will only consider  1000 rows\n",
    "# will get a new different set of DataFrame with 1000 random rows Every time when running the code\n",
    "#df=df.iloc[:10000, :]\n",
    "df=df_old.head(10000).copy()   # Using head for selection, then copy into new dataframe, so that it don't create error in applying apply() at later point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.676317Z",
     "iopub.status.busy": "2024-04-18T12:13:26.675412Z",
     "iopub.status.idle": "2024-04-18T12:13:26.690503Z",
     "shell.execute_reply": "2024-04-18T12:13:26.689479Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.67627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# will need to explore both column one by one\n",
    "# as these two columns are \"Text\" and  \n",
    "# our machine can't understand text so first of all we need to clean our dataset\n",
    "df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.692691Z",
     "iopub.status.busy": "2024-04-18T12:13:26.69225Z",
     "iopub.status.idle": "2024-04-18T12:13:26.699268Z",
     "shell.execute_reply": "2024-04-18T12:13:26.698409Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.692658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#lots of job Kitchen Staff,Book Keeper,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.700784Z",
     "iopub.status.busy": "2024-04-18T12:13:26.70052Z",
     "iopub.status.idle": "2024-04-18T12:13:26.7105Z",
     "shell.execute_reply": "2024-04-18T12:13:26.709603Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.700761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Title'][65]  # row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.712052Z",
     "iopub.status.busy": "2024-04-18T12:13:26.711795Z",
     "iopub.status.idle": "2024-04-18T12:13:26.722418Z",
     "shell.execute_reply": "2024-04-18T12:13:26.72137Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.712029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Job.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.727095Z",
     "iopub.status.busy": "2024-04-18T12:13:26.726757Z",
     "iopub.status.idle": "2024-04-18T12:13:26.733661Z",
     "shell.execute_reply": "2024-04-18T12:13:26.732684Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.72707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Job.Description'][65] # information about column 1 Job.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.734988Z",
     "iopub.status.busy": "2024-04-18T12:13:26.734698Z",
     "iopub.status.idle": "2024-04-18T12:13:26.742898Z",
     "shell.execute_reply": "2024-04-18T12:13:26.741957Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.734951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# In this in each row in entire Description, we can find a lots of keywords in decription\n",
    "# we will first clean content of both column 'Title','Job.Description'\n",
    "# which means we will remove all other things other than text.\n",
    "# Like STOP WORDS, Special Characters , \\r\\n\\r\\n,  other clutters other than text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:26.744475Z",
     "iopub.status.busy": "2024-04-18T12:13:26.744119Z",
     "iopub.status.idle": "2024-04-18T12:13:28.359717Z",
     "shell.execute_reply": "2024-04-18T12:13:28.358556Z",
     "shell.execute_reply.started": "2024-04-18T12:13:26.744443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We will make a function which will clean all these non-essential content from our Title and Description.\n",
    "\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.stem.porter import PorterStemmer  # It is very useful algorithm \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re # regular expression library from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.361935Z",
     "iopub.status.busy": "2024-04-18T12:13:28.361331Z",
     "iopub.status.idle": "2024-04-18T12:13:28.367099Z",
     "shell.execute_reply": "2024-04-18T12:13:28.366023Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.361891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# making object of PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "# Porter Stemmer is an algorithm used in natural language processing (NLP) for stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.36896Z",
     "iopub.status.busy": "2024-04-18T12:13:28.36856Z",
     "iopub.status.idle": "2024-04-18T12:13:28.377765Z",
     "shell.execute_reply": "2024-04-18T12:13:28.376874Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.368925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# making a function to clean The text of Title and Description\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    return cleaned_txt    \n",
    "    # we don't want \\n \\r and special characters, We want text and Integers only and other things will be removed\n",
    "    # we used Substract function  sub() of Regular Expression library(re)\n",
    "   # Inside that using \"\" or'' if content is other than i.e. (Negation of)^ a-z , A-Z or 0-9 and whitespace characters (\\s)\n",
    "# other than these if any other character encounters so don't include them and replace them with '' empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.379049Z",
     "iopub.status.busy": "2024-04-18T12:13:28.378787Z",
     "iopub.status.idle": "2024-04-18T12:13:28.392533Z",
     "shell.execute_reply": "2024-04-18T12:13:28.391652Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.379026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987 3 pages and digit  \\n]\\r\\t $@$%')\n",
    "# we seen above function removed ] and other special characters, but will not able to remove \\r\\n\\t,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.393629Z",
     "iopub.status.busy": "2024-04-18T12:13:28.393393Z",
     "iopub.status.idle": "2024-04-18T12:13:28.403409Z",
     "shell.execute_reply": "2024-04-18T12:13:28.402496Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.393608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    # we don't want \\n \\r, We want text and Integers only and other things will be removed\n",
    "    # we used Substract function  sub() of Regular Expression library(re)\n",
    "   # Inside that using \"\" or'' if content is other than i.e. (Negation of)^ a-z , A-Z or 0-9 \n",
    "# other than these if any other character encounters so don't include them and replace them with '' empty string.\n",
    "# But this can remove special characters, but will not able to remove \\r\\n\\t, we will tokenise\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "# we will tokenise(we will break word-by-word) and will also convert all characters into lower case\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.404885Z",
     "iopub.status.busy": "2024-04-18T12:13:28.404602Z",
     "iopub.status.idle": "2024-04-18T12:13:28.426387Z",
     "shell.execute_reply": "2024-04-18T12:13:28.425567Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.40486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit \\n]\\r\\t $@$%')\n",
    "# tokenized all and converted to lower case, this will also remove \\n \\r \\t,.. in the process of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.428367Z",
     "iopub.status.busy": "2024-04-18T12:13:28.427671Z",
     "iopub.status.idle": "2024-04-18T12:13:28.43375Z",
     "shell.execute_reply": "2024-04-18T12:13:28.432789Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.428325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# now we don't want tokens like 'this', 'is', 'my', 'with','and'\n",
    "# 'copy','0987', '3', 'pages' are good keyword can use\n",
    "\n",
    "# so will remove stopWords, Stopwords are the most frequently occurring words in a language \n",
    "# that carry little or no meaning on their own. They include words like \"the,\" \"a,\" \"an,\" \"is,\" \"of,\" \"and,\" etc.\n",
    "# and will use Stemming by  ps=PorterStemmer() function\n",
    "# Stemming aims to reduce words to their base or root form.\n",
    "#For example, \"running,\" \"runs,\" and \"ran\" would all be stemmed to \"run.\"\n",
    "# The goal is to group similar words together, focusing on their core meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.435047Z",
     "iopub.status.busy": "2024-04-18T12:13:28.434766Z",
     "iopub.status.idle": "2024-04-18T12:13:28.445741Z",
     "shell.execute_reply": "2024-04-18T12:13:28.444897Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.435012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description by also removing StemWords\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "    # using list comprehension \n",
    "    # [for word in tokens if word not in stopwords.words('english')]\n",
    "    # let say tokens have 1,00,000 words so above line will run that number of time\n",
    "    #if these words are not from  then will take  those words in ps.stem() function\n",
    "    stemming = [ps.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    \n",
    "    return stemming    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.446888Z",
     "iopub.status.busy": "2024-04-18T12:13:28.446656Z",
     "iopub.status.idle": "2024-04-18T12:13:28.463175Z",
     "shell.execute_reply": "2024-04-18T12:13:28.462334Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.446868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit moving driven \\n]\\r\\t $@$%')\n",
    "# [for word in tokens if word not in stopwords.words('english')] taken only those words who are not Stopwords\n",
    "# these are stored in word and then given to ps.stem(word)\n",
    "# this makes those non-stopwords into base form by help of ntlk PorterStemmer() function from nltk.stem.porter library by  import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.46433Z",
     "iopub.status.busy": "2024-04-18T12:13:28.464069Z",
     "iopub.status.idle": "2024-04-18T12:13:28.468226Z",
     "shell.execute_reply": "2024-04-18T12:13:28.467408Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.464281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# it is returning a list,  returning it as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.470336Z",
     "iopub.status.busy": "2024-04-18T12:13:28.46953Z",
     "iopub.status.idle": "2024-04-18T12:13:28.476756Z",
     "shell.execute_reply": "2024-04-18T12:13:28.475875Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.470282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description by making change in return line\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "    \n",
    "    stemming = [ps.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    \n",
    "    #return stemming    \n",
    "    return \" \".join(stemming) # joining tokens by separating them with \" \" space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.478353Z",
     "iopub.status.busy": "2024-04-18T12:13:28.477935Z",
     "iopub.status.idle": "2024-04-18T12:13:28.493243Z",
     "shell.execute_reply": "2024-04-18T12:13:28.492335Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.478319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit moving driven \\n]\\r\\t $@$%')\n",
    "# converted into a sigle string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:28.494695Z",
     "iopub.status.busy": "2024-04-18T12:13:28.494432Z",
     "iopub.status.idle": "2024-04-18T12:13:41.772356Z",
     "shell.execute_reply": "2024-04-18T12:13:41.771535Z",
     "shell.execute_reply.started": "2024-04-18T12:13:28.494672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now we seen that it can process on paragraph and find some keywords\n",
    "# now we will pass our title in this function\n",
    "df['Title']= df['Title'].apply( lambda x:cleaning(x))\n",
    "# as Title having 84089 so we will apply lambda, \n",
    "# and x will take each row of Title one by one, and will send it to cleaning() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can give error if our all the data is not of object type, but fortunately we have all data as object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:41.774114Z",
     "iopub.status.busy": "2024-04-18T12:13:41.77378Z",
     "iopub.status.idle": "2024-04-18T12:13:41.782641Z",
     "shell.execute_reply": "2024-04-18T12:13:41.781675Z",
     "shell.execute_reply.started": "2024-04-18T12:13:41.774085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:13:41.784681Z",
     "iopub.status.busy": "2024-04-18T12:13:41.784287Z",
     "iopub.status.idle": "2024-04-18T12:19:59.220624Z",
     "shell.execute_reply": "2024-04-18T12:19:59.219591Z",
     "shell.execute_reply.started": "2024-04-18T12:13:41.78464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df['Job.Description']= df['Job.Description'].apply( lambda x:cleaning(x))\n",
    "# above line gives error that  \"expected string or bytes-like object\"\n",
    "# so before apply() function we will convert our column into string type\n",
    "df['Job.Description']= df['Job.Description'].astype(str).apply( lambda x:cleaning(x))\n",
    "# using astype(str) to convert our column into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.222213Z",
     "iopub.status.busy": "2024-04-18T12:19:59.221875Z",
     "iopub.status.idle": "2024-04-18T12:19:59.230435Z",
     "shell.execute_reply": "2024-04-18T12:19:59.229503Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.222187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Job.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.23737Z",
     "iopub.status.busy": "2024-04-18T12:19:59.237057Z",
     "iopub.status.idle": "2024-04-18T12:19:59.243463Z",
     "shell.execute_reply": "2024-04-18T12:19:59.242579Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.237343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Job.Description'][65] # now if we just see again row 1 does not contain unnecessary tokens or special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.245061Z",
     "iopub.status.busy": "2024-04-18T12:19:59.244752Z",
     "iopub.status.idle": "2024-04-18T12:19:59.263918Z",
     "shell.execute_reply": "2024-04-18T12:19:59.263027Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.245038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['new_col']= df['Title'] +\" \" + df['Job.Description']\n",
    "# making a new 3rd column by joining the old two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.265188Z",
     "iopub.status.busy": "2024-04-18T12:19:59.264888Z",
     "iopub.status.idle": "2024-04-18T12:19:59.279726Z",
     "shell.execute_reply": "2024-04-18T12:19:59.278748Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.265155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.281775Z",
     "iopub.status.busy": "2024-04-18T12:19:59.281049Z",
     "iopub.status.idle": "2024-04-18T12:19:59.287433Z",
     "shell.execute_reply": "2024-04-18T12:19:59.286375Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.281717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# now talking about vectorization\n",
    "# as we are maikng an algorithm for Recommendation system\n",
    "# machine can understand numbers and not text\n",
    "# so first we will try to make vector from it.\n",
    "\n",
    "# 2 things Vector , tfidfvectorizer\n",
    "# we find cosine angle between two vectors if it comes 0, so vectors are too similar and if it comes 90 then these are totally different \n",
    "# sckit-learn provides us cosine\n",
    "# but one more thing need to be done for these vectors-> tfidfvectorizer\n",
    "# tfidfvectorizer -> Term Frequency-Inverse Document Frequency Vectorizer\n",
    "# our row(84,000+) which is containing sentences are also called document.\n",
    "# tfidfvectorizer will help in calculating frequency of each word/token in entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.289013Z",
     "iopub.status.busy": "2024-04-18T12:19:59.288672Z",
     "iopub.status.idle": "2024-04-18T12:19:59.303033Z",
     "shell.execute_reply": "2024-04-18T12:19:59.302078Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.288981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:19:59.304517Z",
     "iopub.status.busy": "2024-04-18T12:19:59.304156Z",
     "iopub.status.idle": "2024-04-18T12:20:10.261047Z",
     "shell.execute_reply": "2024-04-18T12:20:10.259868Z",
     "shell.execute_reply.started": "2024-04-18T12:19:59.304488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# object of TfidfVectorizer\n",
    "tfidf= TfidfVectorizer()\n",
    "#using fit_transform() function \n",
    "matrix=tfidf.fit_transform(df['new_col'])\n",
    "# out of these rows we will find most similar rows/vector by help of cosine_similarity given by scikit learn\n",
    "similarity= cosine_similarity(matrix)\n",
    "similarity  \n",
    "# this will return lists containing values such that it describes similarity of one row values with other remaining rows values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:20:10.262701Z",
     "iopub.status.busy": "2024-04-18T12:20:10.262365Z",
     "iopub.status.idle": "2024-04-18T12:20:10.269985Z",
     "shell.execute_reply": "2024-04-18T12:20:10.268862Z",
     "shell.execute_reply.started": "2024-04-18T12:20:10.262672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "similarity[3]\n",
    "# this give a list showing similarity score all rows which are similar to this row 3 (may be similar tag, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:20:10.271585Z",
     "iopub.status.busy": "2024-04-18T12:20:10.271187Z",
     "iopub.status.idle": "2024-04-18T12:20:10.281062Z",
     "shell.execute_reply": "2024-04-18T12:20:10.280161Z",
     "shell.execute_reply.started": "2024-04-18T12:20:10.271539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function which takes which takes a title input by user\n",
    "def recommendation(title):\n",
    "    try:\n",
    "        idx= df[df['Title'] == title].index[0]  # storing index or rows which having same Title as given by user(title)\n",
    "        #idx=df.index.get_loc(idx) # when indexing is not linearly selected\n",
    "        y= sorted(list(enumerate(similarity[idx])), key=lambda x:x[1], reverse=False) #Sort similarity with indices\n",
    "        top_20= y[1:20]  # Select top 20 similar elements from index 1 (inclusive) to 20 (exclusive)\n",
    "    \n",
    "        jobs=[] # try to store value in list\n",
    "        for idx in top_20:\n",
    "            jobs.append(df.iloc[idx[0]].Title) # storing all 20 similar title in a list one by one\n",
    "        return jobs\n",
    "    \n",
    "    except IndexError:\n",
    "        return \"No recommendations found for this title.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:20:10.282726Z",
     "iopub.status.busy": "2024-04-18T12:20:10.282463Z",
     "iopub.status.idle": "2024-04-18T12:20:10.310875Z",
     "shell.execute_reply": "2024-04-18T12:20:10.30998Z",
     "shell.execute_reply.started": "2024-04-18T12:20:10.282704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recommendation('medic front offic officeteam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T12:20:10.312274Z",
     "iopub.status.busy": "2024-04-18T12:20:10.312003Z",
     "iopub.status.idle": "2024-04-18T12:20:11.712411Z",
     "shell.execute_reply": "2024-04-18T12:20:11.711278Z",
     "shell.execute_reply.started": "2024-04-18T12:20:10.312251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creating simple website for this recommendation system\n",
    "# need to pickle 2 things\n",
    "import pickle\n",
    "pickle.dump(df, open('df.pkl','wb')) # binary mode as write-binary mode(wb)\n",
    "pickle.dump(similarity,open('similarity.pkl','wb')) \n",
    "# created these 2 pickle files 'df.pkl' and 'similarity.pkl' can be used later in website\n",
    "# these 2 files 'df.pkl' and 'similarity.pkl'  are being formed in our Output folder"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 174180,
     "sourceId": 393357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
